{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a864ebe-c984-4a3e-a8c7-80b3a670e057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1842b1f15f54e1a8781fd4a67ae702d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"osunlp/Multimodal-Mind2Web\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c69784f0-fea4-44c7-8197-e6064f70e3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "t = json.load(open(\"test_website_outputs_top50.json\"))\n",
    "test_website_ids = {}\n",
    "\n",
    "for tt in t:\n",
    "    test_website_ids[tt[0]] = tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b650d73b-fd5f-45fb-aa37-ead9203ec7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1018\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "data = []\n",
    "\n",
    "curr_task_id = \"\"\n",
    "curr_action_index = 0\n",
    "for i, item in enumerate(dataset[\"test_website\"]):\n",
    "    \n",
    "    task = item[\"confirmed_task\"]\n",
    "    operation = item[\"operation\"]\n",
    "    raw_html = item[\"raw_html\"]\n",
    "    cleaned_html = item[\"cleaned_html\"]\n",
    "    screenshot = item[\"screenshot\"]\n",
    "    \n",
    "    task_id = item[\"annotation_id\"]\n",
    "    action_id = item[\"action_uid\"]\n",
    "    \n",
    "    combined_id = task_id + \"_\" + action_id\n",
    "    if len(test_website_ids[combined_id]) < 3:\n",
    "        curr_action_index += 1\n",
    "        continue\n",
    "    pos_candidates = test_website_ids[combined_id][2][\"pos_candidates\"]\n",
    "    neg_candidates = test_website_ids[combined_id][2][\"neg_candidates\"]\n",
    "    all_candidates = pos_candidates + neg_candidates\n",
    "    ranked_candidates = sorted(all_candidates, key= lambda x: x[\"rank\"])[:100]\n",
    "    \n",
    "    if task_id != curr_task_id:\n",
    "        curr_task_id = task_id\n",
    "        # data[task_id] = []\n",
    "        curr_action_index = 0\n",
    "    action_sequence = item[\"action_reprs\"]\n",
    "    actions_so_far = action_sequence[:curr_action_index]\n",
    "    curr_action_index += 1\n",
    "    \n",
    "    data.append({\n",
    "        \"action_id\": action_id,\n",
    "        \"task_id\": task_id,\n",
    "        \"pos_candidates\": pos_candidates,\n",
    "        \"ranked_candidates\": ranked_candidates,\n",
    "        \"actions_so_far\": actions_so_far,\n",
    "        \"task\": task,\n",
    "        # \"cleaned_html\": cleaned_html,\n",
    "        # \"raw_html\": raw_html,\n",
    "        \"screenshot\": screenshot,\n",
    "        \"operation\": operation\n",
    "    })\n",
    "    \n",
    "    clear_output()\n",
    "    print(i)\n",
    "    # if i == 100:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d220a509-645c-4578-98df-03577720e621",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_1 = \"\"\"You are DeltAct, a web agent whose job is to imitate humans doing web navigation for a task step by step. \n",
    "At each stage, you can see the webpage like humans by a screenshot and know the previous actions before the current step decided by yourself through recorded history. \n",
    "You need to decide on the first following action to take. You can click an element with the mouse, select an option, or type text with the keyboard. \n",
    "(For your understanding, they are like the click(), select_option() and type() functions in playwright respectively)\n",
    "One next step means one operation within the three.\n",
    "\n",
    "There is some overarching task you are trying to accomplish, and you are currently at some step of the task (could be any step).\n",
    "You will be given the webpage screenshot, the simplified DOM containing some elements of interest, the original task and the sequence of actions taken so far.\n",
    "\n",
    "You need to consider all these and first think through the problem. Think about each of the following areas:\n",
    "\n",
    "(Current Webpage Analysis)\n",
    "Firstly, think about what the current webpage is.\n",
    "\n",
    "(Previous Action Analysis)\n",
    "Secondly, combined with the screenshot, analyze each step of the previous action history and their intention one by one. Particularly, pay more attention to the last step, which may be more related to what you should do now as the next step. Specifically, if the last action involved a TYPE, always evaluate whether it necessitates a confirmation step, because typically a single TYPE action does not make effect. (often, simply pressing 'Enter', assuming the default element involved in the last action, unless other clear elements are present for operation).\n",
    "\n",
    "(Screenshot Details Analysis)\n",
    "Closely examine the screenshot to check the status of every part of the webpage to understand what you can operate with and what has been set or completed. You should closely examine the screenshot details to see what steps have been completed by previous actions even though you are given the textual previous actions. Because the textual history may not clearly and sufficiently record some effects of previous actions, you should closely evaluate the status of every part of the webpage to understand what you have done.\n",
    "\n",
    "(Next Action Based on Webpage and Analysis)\n",
    "Then, based on your analysis, in conjunction with human web browsing habits and the logic of web design, decide on the following action. And clearly outline which element in the webpage users will operate with as the first next target element, its detailed location, and the corresponding operation. Only think through this, don't issue an action right now.\n",
    "\n",
    "Once you have thought through each of these, propose 3 actions that seem most likely to be the correct one. For each action, provide the following:\n",
    "\n",
    "(Proposed Action Element)\n",
    "The index of the element that the action is to be performed on\n",
    "\n",
    "(Proposed Action Op)\n",
    "The operation to be performed on the element. Must be one of CLICK, TYPE, SELECT\n",
    "\n",
    "(Proposed Action Value)\n",
    "Optional value for the operation, e.g., text to type, option to select\n",
    "\n",
    "Then, select one of these 3 to be the Prefinal Action. This is the Action you think is the most likely to be the correct one. Provide the above 3 fields for the Prefinal Action.\n",
    "\n",
    "Then, perform a Validity Check on the Prefinal Action:\n",
    "\n",
    "(Validity Check)\n",
    "Check whether the Action is valid and correct by thinking through it.\n",
    "\n",
    "Then, provide the Final Action in the same format:\n",
    "\n",
    "(Final Action Element)\n",
    "The index of the element that the action is to be performed on\n",
    "\n",
    "(Final Action Op)\n",
    "The operation to be performed on the element. Must be one of CLICK, TYPE, SELECT\n",
    "\n",
    "(Final Action Value)\n",
    "Optional value for the operation, e.g., text to type, option to select\n",
    "\n",
    "Here are the webpage screenshot, the simplified DOM containing some elements of interest (alongside their bounding box coordinates in [left, top, right, bottom] format), the original overarching task and the sequence of actions taken so far (this sequence is described in a somewhat cryptic format):\n",
    "\n",
    "**Simplified DOM**\n",
    "{simplified_dom}\n",
    "\n",
    "**Task**\n",
    "{task}\n",
    "\n",
    "**Actions Taken So Far**\n",
    "{actions_so_far}\n",
    "\"\"\"\n",
    "\n",
    "prompt_2 = \"\"\"You are DeltAct, a web agent whose job is to imitate humans doing web navigation for a task step by step. \n",
    "At each stage, you can see the webpage like humans by a screenshot and know the previous actions before the current step decided by yourself through recorded history. \n",
    "You need to decide on the first following action to take. You can click an element with the mouse, select an option, or type text with the keyboard. \n",
    "(For your understanding, they are like the click(), select_option() and type() functions in playwright respectively)\n",
    "One next step means one operation within the three.\n",
    "\n",
    "There is some overarching task you are trying to accomplish, and you are currently at some step of the task (could be any step).\n",
    "You will be given the webpage screenshot, the simplified DOM containing some elements of interest, the original task and the sequence of actions taken so far.\n",
    "The screenshot has width 700 and height 1100\n",
    "\n",
    "You need to consider all these and first think through the problem. Think about each of the following areas:\n",
    "\n",
    "(Current Webpage Analysis)\n",
    "Firstly, think about what the current webpage is.\n",
    "\n",
    "(Previous Action Analysis)\n",
    "Secondly, combined with the screenshot, analyze each step of the previous action history and their intention one by one. Particularly, pay more attention to the last step, which may be more related to what you should do now as the next step. Specifically, if the last action involved a TYPE, always evaluate whether it necessitates a confirmation step, because typically a single TYPE action does not make effect. (often, simply pressing 'Enter', assuming the default element involved in the last action, unless other clear elements are present for operation).\n",
    "\n",
    "(Screenshot Details Analysis)\n",
    "Closely examine the screenshot to check the status of every part of the webpage to understand what you can operate with and what has been set or completed. You should closely examine the screenshot details to see what steps have been completed by previous actions even though you are given the textual previous actions. Because the textual history may not clearly and sufficiently record some effects of previous actions, you should closely evaluate the status of every part of the webpage to understand what you have done.\n",
    "\n",
    "(Next Action Based on Webpage and Analysis)\n",
    "Then, based on your analysis, in conjunction with human web browsing habits and the logic of web design, decide on the following action. And clearly outline which element in the webpage users will operate with as the first next target element, its detailed location, and the corresponding operation. Only think through this, don't issue an action right now.\n",
    "\n",
    "Once you have thought through each of these, propose the action that seem most likely to be the correct one. For the action, provide the following:\n",
    "\n",
    "(Proposed Action Element X)\n",
    "The X coordinate of the element that the action is to be performed on. Remember that the screenshot has width 700 and height 1100\n",
    "\n",
    "(Proposed Action Element Y)\n",
    "The Y coordinate of the element that the action is to be performed on. Remember that the screenshot has width 700 and height 1100\n",
    "\n",
    "(Proposed Action Op)\n",
    "The operation to be performed on the element. Must be one of CLICK, TYPE, SELECT\n",
    "\n",
    "(Proposed Action Value)\n",
    "Optional value for the operation, e.g., text to type, option to select\n",
    "\n",
    "Then, perform a Validity Check on the Proposed Action:\n",
    "\n",
    "(Validity Check)\n",
    "Check whether the Action is valid and correct by thinking through it.\n",
    "\n",
    "Then, provide the Final Action in the same format:\n",
    "\n",
    "(Final Action Element X)\n",
    "The X coordinate of the element that the action is to be performed on. Remember that the screenshot has width 700 and height 1100\n",
    "\n",
    "(Final Action Element Y)\n",
    "The Y coordinate of the element that the action is to be performed on. Remember that the screenshot has width 700 and height 1100\n",
    "\n",
    "(Final Action Op)\n",
    "The operation to be performed on the element. Must be one of CLICK, TYPE, SELECT\n",
    "\n",
    "(Final Action Value)\n",
    "Optional value for the operation, e.g., text to type, option to select\n",
    "\n",
    "Here are the webpage screenshot, the simplified DOM containing some elements of interest (alongside their bounding box coordinates in [left, top, right, bottom] format), the original overarching task and the sequence of actions taken so far (this sequence is described in a somewhat cryptic format):\n",
    "\n",
    "**Simplified DOM**\n",
    "{simplified_dom}\n",
    "\n",
    "**Task**\n",
    "{task}\n",
    "\n",
    "**Actions Taken So Far**\n",
    "{actions_so_far}\n",
    "\"\"\"\n",
    "\n",
    "# prompt_1_2 = \"\"\"\n",
    "# \"\"\"\n",
    "\n",
    "# prompt_2_1 = \"\"\"You are DeltAct, a web agent whose job is to imitate humans doing web navigation for a task step by step. \n",
    "# At each stage, you can see the webpage like humans by a screenshot and know the previous actions before the current step decided by yourself through recorded history. \n",
    "# You need to decide on the first following action to take. You can click an element with the mouse, select an option, or type text with the keyboard. \n",
    "# (For your understanding, they are like the click(), select_option() and type() functions in playwright respectively)\n",
    "# One next step means one operation within the three.\n",
    "\n",
    "# There is some overarching task you are trying to accomplish, and you are currently at some step of the task (could be any step).\n",
    "# You will be given the webpage screenshot, the simplified DOM containing some elements of interest (alongside their bounding box coordinates in [left, top, right, bottom] format), the original task and the sequence of actions taken so far.\n",
    "\n",
    "# Here are some thoughts and analysis of the current step:\n",
    "\n",
    "# (Current Webpage Analysis)\n",
    "# {current_webpage_analysis}\n",
    "\n",
    "# (Previous Action Analysis)\n",
    "# {previous_action_analysis}\n",
    "\n",
    "# (Screenshot Details Analysis)\n",
    "# {screenshot_details_analysis}\n",
    "\n",
    "# (Next Action Based on Webpage and Analysis)\n",
    "# {next_action_analysis}\n",
    "# \"\"\"\n",
    "\n",
    "# prompt_2_2 = \"\"\"Here are the webpage screenshot, the simplified DOM containing some elements of interest, the original overarching task and the sequence of actions taken so far (this sequence is described in a somewhat cryptic format):\n",
    "\n",
    "# **Simplified DOM**\n",
    "# {simplified_dom}\n",
    "\n",
    "# **Task**\n",
    "# {task}\n",
    "\n",
    "# **Actions Taken So Far**\n",
    "# {actions_so_far}\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52203157-a994-4f0d-8e08-4e2053796754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import instructor\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import supervision as sv\n",
    "from image_utils import batch_elements_by_locality_16_16_17, convert_elements2detections \n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "target_w = 700\n",
    "target_h = 1100\n",
    "\n",
    "class Response1(BaseModel):\n",
    "    current_webpage_analysis: str\n",
    "    previous_action_analysis: str\n",
    "    screenshot_details_analysis: str\n",
    "    next_action_analysis: str\n",
    "    proposed_action_1_element: int\n",
    "    proposed_action_1_op: str\n",
    "    proposed_action_1_value: str\n",
    "    proposed_action_2_element: int\n",
    "    proposed_action_2_op: str\n",
    "    proposed_action_2_value: str\n",
    "    proposed_action_3_element: int\n",
    "    proposed_action_3_op: str\n",
    "    proposed_action_3_value: str\n",
    "    prefinal_action_element: int\n",
    "    prefinal_action_op: str\n",
    "    prefinal_action_value: str\n",
    "    validity_check: str\n",
    "    final_action_element: int\n",
    "    final_action_op: str\n",
    "    final_action_value: str\n",
    "\n",
    "class Response2(BaseModel):\n",
    "    current_webpage_analysis: str\n",
    "    previous_action_analysis: str\n",
    "    screenshot_details_analysis: str\n",
    "    next_action_analysis: str\n",
    "    prefinal_action_element_x: int \n",
    "    prefinal_action_element_y: int \n",
    "    prefinal_action_op: str\n",
    "    prefinal_action_value: str\n",
    "    validity_check: str\n",
    "    final_action_element_x: int\n",
    "    final_action_element_y: int\n",
    "    final_action_op: str\n",
    "    final_action_value: str\n",
    "\n",
    "element_acc = []\n",
    "action_f1 = []\n",
    "step_acc = []\n",
    "\n",
    "def process_and_encode_image(screenshot, ranked_candidates):\n",
    "    \n",
    "    bounding_box_annotator = sv.BoundingBoxAnnotator(\n",
    "        thickness=2\n",
    "    )\n",
    "    candidate_label_annotator = sv.LabelAnnotator(\n",
    "        color_lookup=sv.ColorLookup.INDEX,\n",
    "        text_position=sv.Position.BOTTOM_LEFT,\n",
    "        text_scale=0.5,\n",
    "        text_color=sv.Color.white(),\n",
    "        color=sv.Color.black(),\n",
    "        text_thickness=1\n",
    "    )\n",
    "    \n",
    "    candidate_detections = convert_elements2detections(ranked_candidates)\n",
    "    candidate_labels = [str(i) for i in range(len(candidate_detections))]\n",
    "    \n",
    "    annotated_image = bounding_box_annotator.annotate(scene=np.array(screenshot), detections=candidate_detections)\n",
    "    annotated_image = candidate_label_annotator.annotate(scene=annotated_image, detections=candidate_detections, labels=candidate_labels)\n",
    "    screenshot = Image.fromarray(annotated_image)\n",
    "    screenshot = screenshot.resize((target_w, target_h))\n",
    "    screenshot.save(\"temp.jpg\")\n",
    "    with open(\"temp.jpg\", \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def get_simplified_dom(ranked_candidates, scale_x, scale_y):\n",
    "    simplified_dom = []\n",
    "    \n",
    "    for candidate in ranked_candidates:\n",
    "        attributes = json.loads(candidate[\"attributes\"])\n",
    "        if \"backend_node_id\" in attributes:\n",
    "            del attributes[\"backend_node_id\"]\n",
    "        if \"class\" in attributes:\n",
    "            del attributes[\"class\"]\n",
    "        if \"data_pw_testid_buckeye_candidate\" in attributes:\n",
    "            del attributes[\"data_pw_testid_buckeye_candidate\"]\n",
    "        if \"is_original_target\" in attributes:\n",
    "            del attributes[\"is_original_target\"]\n",
    "        if \"is_top_level_target\" in attributes:\n",
    "            del attributes[\"is_top_level_target\"]\n",
    "        \n",
    "        bounding_box_rect = attributes[\"bounding_box_rect\"]\n",
    "        x1, y1, w, h = bounding_box_rect.split(\",\")\n",
    "        l = str(int(float(x1)/scale_x))\n",
    "        t = str(int(float(y1)/scale_y))\n",
    "        r = str(int(float(x1)/scale_x) + int(float(w)/scale_x))\n",
    "        b = str(int(float(y1)/scale_y) + int(float(h)/scale_y))\n",
    "        attributes[\"bounding_box_rect\"] = \",\".join([l, t, r, b])\n",
    "        \n",
    "        attribute_str = \"\"\n",
    "        for k in attributes.keys():\n",
    "            attribute_str += \" \" + k + \"=\" + attributes[k]\n",
    "        \n",
    "        current_elem = str(candidate[\"rank\"]) + \". \" + \"<\" + candidate[\"tag\"] + attribute_str +\">\"\n",
    "        simplified_dom.append(current_elem)\n",
    "        \n",
    "    simplified_dom = \"\\n\".join(simplified_dom)\n",
    "    return simplified_dom\n",
    "\n",
    "def deltact_gpt4v(simplified_dom, task, actions_so_far, screenshot):\n",
    "    client = instructor.from_openai(OpenAI(api_key=\"\"))\n",
    "    base64_image = process_and_encode_image(screenshot, ranked_candidates)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo-2024-04-09\",\n",
    "        response_model=Response1,\n",
    "        messages=[\n",
    "            # {\"role\": \"user\", \"content\": prompt_1_1},\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt_1.format(simplified_dom=simplified_dom, task=task, actions_so_far = actions_so_far)},\n",
    "                    {\n",
    "                      \"type\": \"image_url\",\n",
    "                      \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                        \"detail\": \"high\"\n",
    "                      },\n",
    "                    },\n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def deltact_claude_haiku(simplified_dom, task, actions_so_far, screenshot):\n",
    "    client = instructor.from_anthropic(Anthropic(api_key=\"\"))\n",
    "    base64_image = process_and_encode_image(screenshot, ranked_candidates)\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        max_tokens=1024,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt_1.format(simplified_dom=simplified_dom, task=task, actions_so_far = actions_so_far)\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"source\": {\n",
    "                            \"type\": \"base64\",\n",
    "                            \"media_type\": \"image/jpeg\",\n",
    "                            \"data\": base64_image,\n",
    "                        },\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        response_model = Response1\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def deltact_claude_sonnet(simplified_dom, task, actions_so_far, screenshot):\n",
    "    client = instructor.from_anthropic(Anthropic(api_key=\"\"))\n",
    "    base64_image = process_and_encode_image(screenshot, ranked_candidates)\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-sonnet-20240229\",\n",
    "        max_tokens=1024,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt_1.format(simplified_dom=simplified_dom, task=task, actions_so_far = actions_so_far)\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"source\": {\n",
    "                            \"type\": \"base64\",\n",
    "                            \"media_type\": \"image/jpeg\",\n",
    "                            \"data\": base64_image,\n",
    "                        },\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        response_model = Response1\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def deltactv2_gpt4v(simplified_dom, task, actions_so_far, screenshot):\n",
    "    client = instructor.from_openai(OpenAI(api_key=\"\"))\n",
    "    base64_image = process_and_encode_image(screenshot, ranked_candidates)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo-2024-04-09\",\n",
    "        response_model=Response2,\n",
    "        messages=[\n",
    "            # {\"role\": \"user\", \"content\": prompt_1_1},\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt_2.format(simplified_dom=simplified_dom, task=task, actions_so_far = actions_so_far)},\n",
    "                    {\n",
    "                      \"type\": \"image_url\",\n",
    "                      \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                        \"detail\": \"high\"\n",
    "                      },\n",
    "                    },\n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4fb3df79-ebae-45d1-b731-1494155daa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = data[0]\n",
    "\n",
    "# process_and_encode_image(temp[\"screenshot\"], ranked_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9baeeb38-4b24-4deb-b20f-cb0e9b2661d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate(results):\n",
    "    \n",
    "    element_acc = []\n",
    "    action_acc = []\n",
    "    step_acc = []\n",
    "    \n",
    "    for result in results:\n",
    "        pos_candidates = result[\"expected\"][\"pos_candidates\"]\n",
    "        op = json.loads(result[\"expected\"][\"operation\"])[\"op\"]\n",
    "        response_element = result[\"response\"][\"final_action_element\"]\n",
    "        response_op = result[\"response\"][\"final_action_op\"]\n",
    "\n",
    "        pos_ranks = [candidate[\"rank\"] for candidate in pos_candidates]\n",
    "        if response_element in pos_ranks:\n",
    "            element_acc.append(1)\n",
    "        else:\n",
    "            element_acc.append(0)\n",
    "        if response_op == op:\n",
    "            action_acc.append(1)\n",
    "        else:\n",
    "            action_acc.append(0)\n",
    "        if element_acc[-1] == 1 and action_acc[-1] == 1:\n",
    "            step_acc.append(1)\n",
    "        else:\n",
    "            step_acc.append(0)\n",
    "    \n",
    "    return {\n",
    "        \"element_acc\": np.mean(element_acc),\n",
    "        \"action_f1\": np.mean(action_acc),\n",
    "        \"step_sr\": np.mean(step_acc)\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluatev2(results):\n",
    "    \n",
    "    element_acc = []\n",
    "    action_acc = []\n",
    "    step_acc = []\n",
    "    \n",
    "    for result in results:\n",
    "        pos_candidates = result[\"expected\"][\"pos_candidates\"]\n",
    "        op = json.loads(result[\"expected\"][\"operation\"])[\"op\"]\n",
    "        response_element = (result[\"response\"][\"final_action_element_x\"], result[\"response\"][\"final_action_element_y\"])\n",
    "        response_op = result[\"response\"][\"final_action_op\"]\n",
    "\n",
    "        pos_ranks = [candidate[\"rank\"] for candidate in pos_candidates]\n",
    "        element_acc.append(0)\n",
    "        for pos_candidate in pos_candidates:\n",
    "            attributes = json.loads(pos_candidate[\"attributes\"])\n",
    "            bounding_box_rect = attributes[\"bounding_box_rect\"]\n",
    "            x1, y1, w, h = bounding_box_rect.split(\",\")\n",
    "            x1 = float(x1) / result[\"scale_x\"]\n",
    "            y1 = float(y1) / result[\"scale_y\"]\n",
    "            x_dist = abs(response_element[0] - x1) / target_w\n",
    "            y_dist = abs(response_element[1] - y1) / target_h\n",
    "            if x_dist < 0.1 and y_dist < 0.1:\n",
    "                element_acc[-1] = 1\n",
    "                break\n",
    "        if response_op == op:\n",
    "            action_acc.append(1)\n",
    "        else:\n",
    "            action_acc.append(0)\n",
    "        if element_acc[-1] == 1 and action_acc[-1] == 1:\n",
    "            step_acc.append(1)\n",
    "        else:\n",
    "            step_acc.append(0)\n",
    "    \n",
    "    return {\n",
    "        \"element_acc\": np.mean(element_acc),\n",
    "        \"action_f1\": np.mean(action_acc),\n",
    "        \"step_sr\": np.mean(step_acc)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626e39d9-155c-45db-a890-8131e2ab1126",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "for item in data:\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    task = item[\"task\"]\n",
    "    operation = item[\"operation\"]\n",
    "    screenshot = item[\"screenshot\"]\n",
    "    actions_so_far = \"\\n\".join(item[\"actions_so_far\"])\n",
    "    ranked_candidates = item[\"ranked_candidates\"][:50]\n",
    "    \n",
    "    scale_x = screenshot.size[0] / target_w\n",
    "    scale_y = screenshot.size[1] / target_h\n",
    "    \n",
    "    simplified_dom = get_simplified_dom(ranked_candidates, scale_x, scale_y)\n",
    "    \n",
    "    try:\n",
    "        response = deltactv2_gpt4v(simplified_dom, task, actions_so_far, screenshot)\n",
    "        response = response.dict()\n",
    "\n",
    "        results.append({\n",
    "            \"expected\": {\n",
    "                \"pos_candidates\": item[\"pos_candidates\"],\n",
    "                \"operation\": operation\n",
    "            },\n",
    "            \"response\": response,\n",
    "            \"scale_x\": scale_x,\n",
    "            \"scale_y\": scale_y\n",
    "        })\n",
    "\n",
    "        # while 1:\n",
    "        #     curr_time = time.time()\n",
    "        #     if curr_time - start_time >= 20:\n",
    "        #         break\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "    \n",
    "    clear_output()\n",
    "    print(len(results))\n",
    "    print(evaluatev2(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45117fb2-628d-41da-8448-229f4f0c8931",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluatev2(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a66ea1f-cb2c-4230-98cf-f284591f8cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
